{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lsa.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak345/OCR-summariser/blob/master/lsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q-9P9mYPihx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqdNG3WnRwoy",
        "colab_type": "code",
        "outputId": "78a17c02-7b7c-4775-d3a9-c918bb292700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpR8qsk6QhGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcmBLwUxRYM_",
        "colab_type": "code",
        "outputId": "8c18fca0-8f0e-4fd9-eb3f-509ff5b79ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data = \"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Paragraphs are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\"\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Paragraphs are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFFYLrMNRlh3",
        "colab_type": "code",
        "outputId": "a6e52a25-7ac8-4e24-ffbe-88c61505f752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "docs = sent_tokenize(data)\n",
        "text = docs\n",
        "text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.',\n",
              " 'LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).',\n",
              " 'A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.',\n",
              " 'Paragraphs are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns.',\n",
              " 'Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ5CHxr8lLHv",
        "colab_type": "code",
        "outputId": "96c4e062-9e8a-4eb8-a7b1-72ceff9e983e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.DataFrame(docs, columns=['sent'])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Latent semantic analysis (LSA) is a technique ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LSA assumes that words that are close in meani...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A matrix containing word counts per paragraph ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Paragraphs are then compared by taking the cos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Values close to 1 represent very similar parag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sent\n",
              "0  Latent semantic analysis (LSA) is a technique ...\n",
              "1  LSA assumes that words that are close in meani...\n",
              "2  A matrix containing word counts per paragraph ...\n",
              "3  Paragraphs are then compared by taking the cos...\n",
              "4  Values close to 1 represent very similar parag..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lZm3b-B_Szs",
        "colab_type": "code",
        "outputId": "adb19ff7-8039-4880-e2bc-99e4dec57068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtXovP4R_OWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5NLFx8J_PEh",
        "colab_type": "code",
        "outputId": "7c0c7564-2f13-4ea9-d10a-9bec713d1d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df['sent'] = df['sent'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "df['sent'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Latent semantic analysis (LSA) technique natural language processing, particular distributional semantics, analyzing relationships set documents terms contain producing set concepts related documents terms.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD4zXNznRs1a",
        "colab_type": "code",
        "outputId": "ed05a706-6f1c-4819-d10d-106ccb1dcda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 2, smooth_idf = True)\n",
        "vectorizer.fit(df['sent'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
              "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwnZoSx7iTvJ",
        "colab_type": "code",
        "outputId": "1f746b9c-33f9-4d8b-ea90-25cde5fcf877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lsa': 3, 'technique': 7, 'distributional': 2, 'words': 9, 'close': 0, 'similar': 6, 'text': 8, 'represent': 5, 'columns': 1, 'paragraphs': 4}\n",
            "[1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.69314718 1.69314718 1.69314718]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPB7kSPniXwS",
        "colab_type": "code",
        "outputId": "f24528a0-1ab1-487b-f1c1-97af2fd746fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vector = vectorizer.transform(df['sent'])\n",
        "# print(vector)\n",
        "print(vector.shape)\n",
        "tfidf = vector.toarray()\n",
        "# tfidf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhXs_NomY0N7",
        "colab_type": "code",
        "outputId": "d2a468d8-1af5-4179-f196-359b14e0eafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tfidf = tfidf.T\n",
        "print(tfidf.shape)\n",
        "print(tfidf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5)\n",
            "[[0.         0.40824829 0.         0.         0.5547002 ]\n",
            " [0.         0.         0.60302269 0.70710678 0.        ]\n",
            " [0.57735027 0.40824829 0.         0.         0.        ]\n",
            " [0.57735027 0.40824829 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.70710678 0.5547002 ]\n",
            " [0.         0.         0.60302269 0.         0.5547002 ]\n",
            " [0.         0.40824829 0.         0.         0.2773501 ]\n",
            " [0.57735027 0.         0.30151134 0.         0.        ]\n",
            " [0.         0.40824829 0.30151134 0.         0.        ]\n",
            " [0.         0.40824829 0.30151134 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Zg86Re5sW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd_model = TruncatedSVD(n_components = -(-len(text)//2),algorithm='randomized', n_iter=10, random_state=22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScKOIfSxVC2N",
        "colab_type": "code",
        "outputId": "6457629a-f21d-448f-ac90-7c25f11589bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "svd_model.fit(tfidf)\n",
        "\n",
        "svd_model.n_components"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJjFsLqeVba1",
        "colab_type": "code",
        "outputId": "e00f2c86-f970-47b6-b43a-7b56c911791a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "svd_model.components_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.30735003,  0.45000746,  0.52083595,  0.42495099,  0.50117444],\n",
              "       [ 0.62984917,  0.51310073, -0.17410579, -0.50185479, -0.24051322],\n",
              "       [-0.41432952,  0.356272  , -0.42650899, -0.31780859,  0.64690733]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTHG7xs-iXDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc = svd_model.components_\n",
        "svc = svc.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdRMv5-5ikac",
        "colab_type": "code",
        "outputId": "25fefbd3-41b3-46ed-cffc-d9be8523ca66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "svc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.3073500337440256,\n",
              "  0.4500074569621447,\n",
              "  0.520835947161369,\n",
              "  0.42495098535202247,\n",
              "  0.5011744423141258],\n",
              " [0.6298491686324654,\n",
              "  0.5131007328141923,\n",
              "  -0.17410578876022723,\n",
              "  -0.5018547874025349,\n",
              "  -0.24051322092530797],\n",
              " [-0.4143295175773899,\n",
              "  0.35627199632993245,\n",
              "  -0.426508987538849,\n",
              "  -0.31780859055121163,\n",
              "  0.6469073340252146]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hiL3bRThZpD",
        "colab_type": "code",
        "outputId": "58b8ca23-ef03-4fae-d513-214f950fb9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "summary = ''\n",
        "p_pos = 0\n",
        "for _ in range(len(svc)):\n",
        "  m = max(svc[_])\n",
        "  print(m)\n",
        "  pos = svc[_].index(m)\n",
        "  print(pos)\n",
        "  print(text[pos])\n",
        "  if pos > p_pos:\n",
        "    summary = summary + '\\n' + text[pos]\n",
        "  else:\n",
        "    summary = text[pos] + '\\n' + summary  \n",
        "  p_pos = pos\n",
        "print(summary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.520835947161369\n",
            "2\n",
            "A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.\n",
            "0.6298491686324654\n",
            "0\n",
            "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
            "0.6469073340252146\n",
            "4\n",
            "Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\n",
            "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
            "\n",
            "A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.\n",
            "Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U14FFXyBCYeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWSCBTcN3KXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKW7EtBD3PVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(data):\n",
        "  text = sent_tokenize(data)\n",
        "  df = pd.DataFrame(text, columns=['sent'])\n",
        "\n",
        "  stop = stopwords.words('english')\n",
        "  df['sent'] = df['sent'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  return text, df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUfdy_WY7ilx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summary(text, df) :\n",
        "  vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 2, smooth_idf = True)\n",
        "  vectorizer.fit(df['sent'])\n",
        "  vector = vectorizer.transform(df['sent'])\n",
        "  tfidf = vector.toarray()\n",
        "  tfidf = tfidf.T\n",
        "\n",
        "  svd_model = TruncatedSVD(n_components = -(-len(text)//2),algorithm='randomized', n_iter=10, random_state=22)\n",
        "  svd_model.fit(tfidf)\n",
        "  svc = svd_model.components_\n",
        "  svc = svc.tolist()\n",
        "\n",
        "  summary = ''\n",
        "  p_pos = 0\n",
        "  for _ in range(len(svc)):\n",
        "    m = max(svc[_])\n",
        "    # print(m)\n",
        "    pos = svc[_].index(m)\n",
        "    # print(pos)\n",
        "    # print(text[pos])\n",
        "    if pos > p_pos:\n",
        "      summary = summary + '\\n' + text[pos]\n",
        "    else:\n",
        "      summary = text[pos] + '\\n' + summary  \n",
        "    p_pos = pos\n",
        "  return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1sGW_Gd9_we",
        "colab_type": "code",
        "outputId": "3b7e43b5-97f0-490c-e64b-960708ccde27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "data = \"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Paragraphs are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two columns. Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\"\n",
        "\n",
        "text,df = preprocessing(data)\n",
        "summary = summary(text, df)\n",
        "\n",
        "print(summary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
            "\n",
            "A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.\n",
            "Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NA4pRnuASCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}