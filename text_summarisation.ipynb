{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "text_summarisation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak345/OCR-summariser/blob/master/text_summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmwGYazTQvjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbb99eb0-e9b6-44cb-ac0a-68e4115b3971"
      },
      "source": [
        "!mkdir minor_project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘minor_project’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwyjW3rZO9LG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c870f814-ed3d-41d6-bc84-e966d03d3077"
      },
      "source": [
        "%cd minor_project/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/minor_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Eago6CYhb-nt",
        "colab_type": "code",
        "outputId": "636f4b70-b933-40e6-9241-714933f1d65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "  ! pip install git+https://github.com/miso-belica/sumy.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/miso-belica/sumy.git\n",
            "  Cloning https://github.com/miso-belica/sumy.git to /tmp/pip-req-build-y0te44gg\n",
            "  Running command git clone -q https://github.com/miso-belica/sumy.git /tmp/pip-req-build-y0te44gg\n",
            "Requirement already satisfied (use --upgrade to upgrade): sumy==0.8.1 from git+https://github.com/miso-belica/sumy.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy==0.8.1) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.6/dist-packages (from sumy==0.8.1) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy==0.8.1) (2.21.0)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.6/dist-packages (from sumy==0.8.1) (19.8.18)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy==0.8.1) (3.2.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy==0.8.1) (4.2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy==0.8.1) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy==0.8.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy==0.8.1) (1.12.0)\n",
            "Building wheels for collected packages: sumy\n",
            "  Building wheel for sumy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sumy: filename=sumy-0.8.1-py2.py3-none-any.whl size=82986 sha256=9d6b30f81cf353563055de5d3a29e1bc61b55fd52f7bccf29afd511d2b18c756\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q5fmwqbs/wheels/a3/4f/24/b64d78315f591ef9b7cdcdf3a70bd060d0475ec927c0d914a0\n",
            "Successfully built sumy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "9piGsCuNb-ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from gensim.summarization import summarize\n",
        "from sumy.utils import get_stop_words\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as sumytoken\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.utils import get_stop_words\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as sumytoken\n",
        "\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "from sumy.utils import get_stop_words\n",
        "from sumy.nlp.stemmers import Stemmer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer as sumytoken\n",
        "\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "\n",
        "\n",
        "def gensim_summarizer(text):\n",
        "    return (summarize(text))\n",
        "\n",
        "def lexrank_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT):\n",
        "    parser = PlaintextParser.from_string((text), sumytoken(LANGUAGE))\n",
        "    summarizer_LexRank = LexRankSummarizer(stemmer)\n",
        "    summarizer_LexRank.stop_words = get_stop_words(LANGUAGE)\n",
        "    sentences = []\n",
        "    for sentence in summarizer_LexRank(parser.document, SENTENCES_COUNT):\n",
        "        a = sentence\n",
        "        sentences.append(str(a))\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "def lsa_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT):\n",
        "    parser = PlaintextParser.from_string((text), sumytoken(LANGUAGE))\n",
        "    summarizer_lsa = Summarizer(stemmer)\n",
        "    summarizer_lsa.stop_words = get_stop_words(LANGUAGE)\n",
        "    sentences = []\n",
        "    for sentence in summarizer_lsa(parser.document, SENTENCES_COUNT):\n",
        "        a = sentence\n",
        "        sentences.append(str(a))\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "def luhn_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT):\n",
        "    parser = PlaintextParser.from_string(text, sumytoken(LANGUAGE))\n",
        "    summarizer_luhn = LuhnSummarizer(stemmer)\n",
        "    summarizer_luhn.stop_words = get_stop_words(LANGUAGE)\n",
        "    sentences = []\n",
        "    for sentence in summarizer_luhn(parser.document, SENTENCES_COUNT):\n",
        "        a = sentence\n",
        "        sentences.append(str(a))\n",
        "    return \" \".join(sentences)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVeFpLIXfub5",
        "colab_type": "code",
        "outputId": "dc430adb-12cb-4b78-8052-b4f92cb7eb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ayt_379Tb-n3",
        "colab_type": "code",
        "outputId": "c8757e1c-f31c-4c0a-ca41-0d5a267600ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sumy.nlp.stemmers import Stemmer\n",
        "\n",
        "LANGUAGE = \"english\"\n",
        "SENTENCES_COUNT = 2\n",
        "stemmer = Stemmer(LANGUAGE)\n",
        "text = 'The contribution of cloud computing and mobile computing technologies lead to the newly emerging mobile cloud com- puting paradigm. Three major approaches have been pro- posed for mobile cloud applications: 1) extending the access to cloud services to mobile devices; 2) enabling mobile de- vices to work collaboratively as cloud resource providers; 3) augmenting the execution of mobile applications on portable devices using cloud resources. In this paper, we focus on the third approach in supporting mobile data stream applica- tions. More specifically, we study how to optimize the com- putation partitioning of a data stream application between mobile and cloud to achieve maximum speed/throughput in processing the streaming data. To the best of our knowledge, it is the first work to study the partitioning problem for mobile data stream applica- tions, where the optimization is placed on achieving high throughput of processing the streaming data rather than minimizing the makespan of executions as in other appli- cations. We first propose a framework to provide runtime support for the dynamic computation partitioning and exe- cution of the application. Different from existing works, the framework not only allows the dynamic partitioning for a single user but also supports the sharing of computation in- stances among multiple users in the cloud to achieve efficient utilization of the underlying cloud resources. Meanwhile, the framework has better scalability because it is designed on the elastic cloud fabrics. Based on the framework, we design a genetic algorithm for optimal computation parti- tion. Both numerical evaluation and real world experiment have been performed, and the results show that the par- titioned application can achieve at least two times better performance in terms of throughput than the application without partitioning.'\n",
        "\n",
        "gensim_summary = gensim_summarizer(text)\n",
        "lexrank_summary = lexrank_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT)\n",
        "lsa_summary = lsa_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT)\n",
        "luhn_summary = luhn_summarizer(text, stemmer, LANGUAGE, SENTENCES_COUNT)\n",
        "\n",
        "print (\"\\n ===GENSIM===\\n\",gensim_summary)\n",
        "print (\"\\n ===Lexrank==\\n\",lexrank_summary)\n",
        "print (\"\\n ===LSA==\\n\",lsa_summary)\n",
        "print (\"\\n ===luhn==\\n\",luhn_summary)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===GENSIM===\n",
            " More specifically, we study how to optimize the com- putation partitioning of a data stream application between mobile and cloud to achieve maximum speed/throughput in processing the streaming data.\n",
            "To the best of our knowledge, it is the first work to study the partitioning problem for mobile data stream applica- tions, where the optimization is placed on achieving high throughput of processing the streaming data rather than minimizing the makespan of executions as in other appli- cations.\n",
            "\n",
            " ===Lexrank==\n",
            " Three major approaches have been pro- posed for mobile cloud applications: 1) extending the access to cloud services to mobile devices; 2) enabling mobile de- vices to work collaboratively as cloud resource providers; 3) augmenting the execution of mobile applications on portable devices using cloud resources. In this paper, we focus on the third approach in supporting mobile data stream applica- tions.\n",
            "\n",
            " ===LSA==\n",
            " We first propose a framework to provide runtime support for the dynamic computation partitioning and exe- cution of the application. Different from existing works, the framework not only allows the dynamic partitioning for a single user but also supports the sharing of computation in- stances among multiple users in the cloud to achieve efficient utilization of the underlying cloud resources.\n",
            "\n",
            " ===luhn==\n",
            " Three major approaches have been pro- posed for mobile cloud applications: 1) extending the access to cloud services to mobile devices; 2) enabling mobile de- vices to work collaboratively as cloud resource providers; 3) augmenting the execution of mobile applications on portable devices using cloud resources. To the best of our knowledge, it is the first work to study the partitioning problem for mobile data stream applica- tions, where the optimization is placed on achieving high throughput of processing the streaming data rather than minimizing the makespan of executions as in other appli- cations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqJgLHDse1_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}